name: Visual Regression Tests

on:
  pull_request:
    types: [opened, synchronize, reopened]
  workflow_dispatch:

jobs:
  visual-tests:
    runs-on: ubuntu-latest

    permissions:
      contents: read
      checks: write
      pull-requests: write

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set reusable strings
      id: strings
      shell: bash
      run: |
        echo "build-output-dir=${{ github.workspace }}/build" >> "$GITHUB_OUTPUT"

    - name: Install Dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          build-essential cmake \
          libglfw3-dev libglm-dev \
          mesa-utils xvfb \
          libgl1-mesa-dev libglu1-mesa-dev

    - name: Configure CMake
      run: |
        cmake -B ${{ steps.strings.outputs.build-output-dir }} \
          -DCMAKE_CXX_COMPILER=g++ \
          -DCMAKE_C_COMPILER=gcc \
          -DCMAKE_BUILD_TYPE=Release \
          -DBUILD_TESTS=ON \
          -S ${{ github.workspace }}

    - name: Build Tests
      run: |
        cmake --build ${{ steps.strings.outputs.build-output-dir }} \
          --config Release \
          --target ParticleViewerTests

    - name: Run Visual Regression Tests
      id: visual-tests
      working-directory: ${{ steps.strings.outputs.build-output-dir }}/tests
      run: |
        # Run only visual regression fixture tests under Xvfb
        # (VisualRegressionTest produces real diff/baseline/current artifacts)
        xvfb-run -a ./ParticleViewerTests \
          --gtest_filter="VisualRegressionTest.*" \
          --gtest_output=xml:visual-test-results.xml \
          2>&1 | tee visual-test-output.txt

        # Capture exit code
        TEST_EXIT_CODE=${PIPESTATUS[0]}
        echo "test_exit_code=${TEST_EXIT_CODE}" >> "$GITHUB_OUTPUT"

        # Parse test results for the comment using gtest final summary lines
        TOTAL=$(grep -E '^\[==========\] [0-9]+ tests? from [0-9]+ test suites? ran\.' visual-test-output.txt \
          | tail -1 \
          | grep -oE '[0-9]+' \
          | head -1 || echo "0")
        PASSED=$(grep -E '^\[  PASSED  \] [0-9]+ tests?\.' visual-test-output.txt \
          | tail -1 \
          | grep -oE '[0-9]+' \
          | head -1 || echo "0")
        FAILED=$(grep -E '^\[  FAILED  \] [0-9]+ tests?, listed below:' visual-test-output.txt \
          | tail -1 \
          | grep -oE '[0-9]+' \
          | head -1 || echo "0")

        echo "total_tests=${TOTAL}" >> "$GITHUB_OUTPUT"
        echo "passed_tests=${PASSED}" >> "$GITHUB_OUTPUT"
        echo "failed_tests=${FAILED}" >> "$GITHUB_OUTPUT"

        exit ${TEST_EXIT_CODE}

    - name: Upload Test Results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: visual-regression-results
        path: |
          ${{ steps.strings.outputs.build-output-dir }}/tests/visual-test-results.xml
          ${{ steps.strings.outputs.build-output-dir }}/tests/visual-test-output.txt
        retention-days: 7

    - name: Upload Diff Images on Failure
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: visual-regression-diffs
        path: |
          ${{ steps.strings.outputs.build-output-dir }}/tests/diffs/
          ${{ steps.strings.outputs.build-output-dir }}/tests/artifacts/
        retention-days: 7
        if-no-files-found: ignore

    - name: Comment Visual Diff on PR
      if: always() && github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          const testExitCode = '${{ steps.visual-tests.outputs.test_exit_code }}';
          const total = '${{ steps.visual-tests.outputs.total_tests }}' || '0';
          const passed = '${{ steps.visual-tests.outputs.passed_tests }}' || '0';
          const failed = '${{ steps.visual-tests.outputs.failed_tests }}' || '0';
          const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
          const artifactsDir = '${{ steps.strings.outputs.build-output-dir }}/tests/artifacts';
          const diffsDir = '${{ steps.strings.outputs.build-output-dir }}/tests/diffs';

          const status = testExitCode === '0' ? 'âœ…' : 'âŒ';
          const statusText = testExitCode === '0' ? 'All visual tests passed!' : 'Visual regression detected!';

          let body = `## ðŸŽ¨ Visual Regression Test Report\n\n`;
          body += `**Status:** ${status} ${statusText}\n\n`;
          body += `| Metric | Value |\n`;
          body += `|--------|-------|\n`;
          body += `| Total Tests | ${total} |\n`;
          body += `| Passed | ${passed} |\n`;
          body += `| Failed | ${failed} |\n\n`;

          // Embed images inline if artifacts exist
          const imageFiles = [];
          for (const dir of [artifactsDir, diffsDir]) {
            if (fs.existsSync(dir)) {
              const files = fs.readdirSync(dir).filter(f => f.endsWith('.png'));
              for (const file of files) {
                const filePath = path.join(dir, file);
                const data = fs.readFileSync(filePath);
                const base64 = data.toString('base64');
                imageFiles.push({ name: file, base64 });
              }
            }
          }

          if (imageFiles.length > 0) {
            body += `### ðŸ“¸ Visual Test Images\n\n`;

            // Group by test name (strip _baseline/_current/_diff suffix)
            const groups = {};
            for (const img of imageFiles) {
              const match = img.name.match(/^(.+?)_(baseline|current|diff)\.png$/);
              if (match) {
                const testName = match[1];
                const type = match[2];
                if (!groups[testName]) groups[testName] = {};
                groups[testName][type] = img;
              } else {
                // Ungrouped image
                if (!groups['_other']) groups['_other'] = {};
                groups['_other'][img.name] = img;
              }
            }

            for (const [testName, images] of Object.entries(groups)) {
              if (testName === '_other') continue;
              body += `#### ${testName}\n\n`;
              body += `| Baseline | Current | Diff |\n`;
              body += `|----------|---------|------|\n`;

              const baseline = images.baseline
                ? `<img src="data:image/png;base64,${images.baseline.base64}" width="200" />`
                : 'N/A';
              const current = images.current
                ? `<img src="data:image/png;base64,${images.current.base64}" width="200" />`
                : 'N/A';
              const diff = images.diff
                ? `<img src="data:image/png;base64,${images.diff.base64}" width="200" />`
                : 'N/A';

              body += `| ${baseline} | ${current} | ${diff} |\n\n`;
            }
          } else if (testExitCode === '0') {
            body += `All images matched their baselines â€” no diffs generated.\n\n`;
          }

          body += `---\n`;
          body += `*Generated by [Visual Regression Tests](${runUrl})*`;

          // Find existing comment to update (don't spam)
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });

          const botComment = comments.find(comment =>
            comment.user.type === 'Bot' &&
            comment.body.includes('Visual Regression Test Report')
          );

          if (botComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: body
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });
          }
